#
# Whether to deploy sylva-core
#
deploy_sylva_core: false

########################################################
# Sylva Configurations                                 #
#                                                      #
# NOTE: these configurations are effective only if     #
# "deploy_sylva_core" is set to "true"                 #
########################################################

#
# Repo which contains the workload cluster manifests
#
# workload_cluster_manifest_repo: https://gitlab.suse.de/cloud-solutions-sys-eng/metal3-clusters

#
# The workload cluster manifest repo branch to use. By default,
# it is set to the "main" branch.
#
# workload_cluster_manifest_repo_branch: main

#
# The path to the chart (in the workload cluster manifests repo above) to use
#
# workload_cluster_chart_path: ./hlm006

#
# If the repo is a private repo and the trusted CA certificate needed in
# order to validate the repo HTTPs certificate. Specify the name of the
# secret which contains that CA certificate.
#
# NOTE: the secret is expected to be created prior
#
# workload_cluster_secret_ref_name: suse-root-ca

##########################
# General Configurations #
##########################

#
# RKE2 channel to install a specific version of RKE2 server.
# This is to ensure that the given RKE2 version is compatible with the latest
# version of Rancher.
#
rke2_channel_version: v1.24

#
# *Your* github person access token, used to interact with github APIs
# during provisioning of the Metal3 stack, if you are consistently running
# into github API rate limiting problems.
#
# NOTE: this is *optional*. You only need to specify your github personal
# access token if you run into github API rate limiting problems.
#
#github_token: <your github personal access token here>

#
# Optional: specify the pull request to use for the
# https:/github.com/rancher-sandbox/baremetal.git repo. By default, the
# latest main branch is used.
#
#baremetal_pull_request: 63

#
# VM user account. This is the default user for all the VMs.
# This is also the Ansible user, used to setup the Metal3 stack
# on the VMs. This user will also have sudo access to the VMs
# per Ansible and Metal3 requirements.
#

# VM user name
vm_user: metal

# VM user plain text password (not hash)
vm_user_plain_text_password: metal

# NOTE: this should be *your* (local user) SSH public key since *you*
# will be using it to login to the VMs. The SSH public keys listed
# here will be appended to the VM user's authorized_keys file.
#
vm_authorized_ssh_keys:
  - <Your ssh public key here>

# OS image
opensuse_leap_image_url: https://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.5/images/openSUSE-Leap-15.5.x86_64-NoCloud.qcow2
opensuse_leap_image_checksum: sha256:https://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.5/images/openSUSE-Leap-15.5.x86_64-NoCloud.qcow2.sha256
opensuse_leap_image_name: openSUSE-Leap-15.5.x86_64-NoCloud.qcow2

#########################################
# Network Infrastructure Configurations #
#########################################

# NOTE: the Metal3 demo environment is consisted of two VMs, 
# metal3-network-infra and metal3-core respectively. The metal3-network-infra
# VM contains all necessary networking infrastructure components that
# Metal3 stack dependeds on, which are expected to be available in a
# typical production environment. i.e. DNS server, DHCP server, etc. 
# The metal3-core VM, on the other hand, has only the essential CAPI and
# Metal3 components. i.e. CAPI, RKE2 bootstrap and control plane provider,
# Metal3 infrastructure provider, baremetal operator, and Ironic.
# The external-dns in metal3-core is configured to utilize the DNS server
# running in metal3-network-infra. Likewise, metal3-core is leveraging the
# DNS server in metal3-network-infra for all DNS resolutions.
#
# The Metal3 demo environment is expected to have two networks, which are
# provisioning and public network. The provisioning network is used
# for provisioning, which means it must be able to access the bare metal
# computers (BMC). For all intents and purposes, the provisioning network
# is the same as the iLO network (or has route to the iLO network).
# The public network, as the name implies, should have access (egress)
# to the internet. It is used to download bits from public repos
# or endpoints during cluster provisioning. And it can also be used
# to access the workload cluster API endpoints.
#
# For same environments, though not recommended for obviouse reasons,
# provisioning and public network can be the same so as long as the
# network have public access.

#
# DNS domain for the Metal3 stack. All the public endpoints created in DNS
# will have this domain. e.g. boot.ironic.suse.baremetal, media.suse.baremetal, etc
#
dns_domain: suse.baremetal

#
# NIC for the provisioning network
#
# NOTE: Please configure the NIC after the "&metal3_provisioning_nic"
# part because we need this reference in the VM network configuration below.
#
metal3_provisioning_nic: &metal3_provisioning_nic eth0

#
# Host to VM NIC mapping, determines how to map host NICs to 
# VM NICs. This is the "--network" parameters for virt-install
# when creating the VMs.
#
metal3_vm_libvirt_network_params: '--network bridge=virbr0,model=virtio --network bridge=br-eth3,model=virtio'

###################################################################
# Configurations specific to the Metal3 Network Infrastructure VM #
###################################################################

#
# Provisioning IP
#
metal3_network_infra_provisioning_ip: 192.168.122.8

#
# Public IP
#
metal3_network_infra_public_ip: 10.84.56.112

#
# Whether to enable DHCP server. It is enabled by default.
#
enable_dhcp: true

#
# Must configure the following if DHCP server is enabled.
#
#dhcp_router: 192.168.122.1
#dhcp_range: 192.168.122.100,192.168.122.130


#
# Networking configurations for the Metal3 Network Infrastructure VM
#
# NOTE: the network configuration is expected to be specified in
# cloud-init network-config format as the data will be passed into
# cloud-init as is when setting up the VM.
# See https://cloudinit.readthedocs.io/en/latest/reference/network-config-format-v2.html
#
metal3_network_infra_vm_network:
  version: 2
  ethernets:
    *metal3_provisioning_nic:
      dhcp4: false
      addresses: ["{{ metal3_network_infra_provisioning_ip }}/24"]
      nameservers:
        addresses: [8.8.8.8]
        search:
          - "{{ dns_domain }}"
      routes:
        - to: 192.168.122.0/24
          via: 192.168.122.1
    eth1:
      dhcp4: false
  vlans:
    eth1.824:
      id: 824
      link: eth1
      addresses: ["{{ metal3_network_infra_public_ip }}/21"]
      nameservers:
        addresses: [8.8.8.8]
        search:
          - "{{ dns_domain }}"
      routes:
        - to: default
          via: 10.84.63.254


#################################################
# Configurations specific to the Metal3 Core VM #
#################################################

#
# Provisioning IP
#
metal3_core_provisioning_ip: 192.168.122.9

#
# Public IP
#
metal3_core_public_ip: 10.84.56.113

#
# Networking configurations for the Metal3 Core VM
#
# NOTE: the network configuration is expected to be specified in
# cloud-init network-config format as the data will be passed into
# cloud-init as is when setting up the VM.
# See https://cloudinit.readthedocs.io/en/latest/reference/network-config-format-v2.html
#
metal3_core_vm_network:
  version: 2
  ethernets:
    *metal3_provisioning_nic:
      dhcp4: false
      addresses: ["{{ metal3_core_provisioning_ip }}/24"]
      nameservers:
        addresses: ["{{ dns_server }}"]
        search:
          - "{{ dns_domain }}"
      routes:
        - to: 192.168.122.0/24
          via: 192.168.122.1
    eth1:
      dhcp4: false
  vlans:
    eth1.824:
      id: 824
      link: eth1
      addresses: ["{{ metal3_core_public_ip }}/21"]
      nameservers:
        addresses: ["{{ dns_server }}"]
        search:
          - "{{ dns_domain }}"
      routes:
        - to: default
          via: 10.84.63.254

# Storage setup on the Metal3 Core VM
storage:
  class_name: dynamic
  access_mode: ReadWriteMany
  nfs:
    create: true
    path: "/nfs/share"
